{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigram Homework\n",
    "\n",
    "Exercises:\n",
    "E01: train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F \n",
    "\n",
    "names = open('names.txt', 'r').read().splitlines()\n",
    "stoi = {ch:i+1 for i, ch in enumerate(sorted(list(set(''.join(names)))))}\n",
    "stoi['.'] = 0\n",
    "itos = {i:ch for ch, i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting method\n",
    "# make dataset\n",
    "class Counting_Trigrams:\n",
    "    def __init__(self, name_list):\n",
    "        self.name_list = name_list\n",
    "        self.N, self.P = self.trigram_counting_p_dist()\n",
    "        self.loss = self.counting_p_loss()\n",
    "\n",
    "    def trigram_counting_p_dist(self):\n",
    "        N = torch.zeros((27,27,27), dtype = torch.int32)\n",
    "        for name in self.name_list:\n",
    "            name = ['.', '.'] + list(name) + ['.']\n",
    "            for i in range(len(name) - 2):\n",
    "                N[stoi[name[i]], stoi[name[i+1]], stoi[name[i+2]]] += 1\n",
    "        P = (N).float()\n",
    "        P /= P.sum(2, keepdim=True)         #this was hard.... tried 1, and omitting keepdim\n",
    "        return N, P\n",
    "\n",
    "    def counting_p_loss(self):\n",
    "        log_likelihood = 0\n",
    "        num_trigrams = 0\n",
    "        for name in self.name_list:\n",
    "            name = ['.', '.'] + list(name) + ['.']\n",
    "            for i in range(len(name) - 2):\n",
    "                log_likelihood += self.P[stoi[name[i]], stoi[name[i+1]], stoi[name[i+2]]].log()\n",
    "                num_trigrams += 1\n",
    "        loss = - log_likelihood/num_trigrams\n",
    "        return loss\n",
    "\n",
    "    def get_names(self,num_names):\n",
    "        names = []\n",
    "        for i in range(num_names):\n",
    "            iix = ix = 0\n",
    "            out = []\n",
    "            while True:\n",
    "                p = self.P[iix, ix]       # same result, this works\n",
    "                x = torch.multinomial(p, num_samples=1, replacement=True).item()\n",
    "                out.append(itos[x])\n",
    "                if x == 0:\n",
    "                    break\n",
    "                iix, ix = ix, x\n",
    "            names.append(''.join(out[:-1]))\n",
    "        return names\n",
    "    \n",
    "    def get_P_bigrams(self, ch1, ch2):\n",
    "        prob = self.P[stoi[ch1], stoi[ch2]]\n",
    "        return prob\n",
    "    \n",
    "    def get_N_bigrams(self, ch1, ch2):\n",
    "        count = self.N[stoi[ch1], stoi[ch2]]\n",
    "        return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.1857)\n",
      "['itraci', 'kayan', 'nin', 'ma', 'fya', 'noutchaibanatse', 'palodna', 'mia', 'ni', 'magar']\n"
     ]
    }
   ],
   "source": [
    "whole_set = Counting_Trigrams(names)\n",
    "print(whole_set.loss)\n",
    "some_names = whole_set.get_names(10)\n",
    "print(some_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, '.': 0}\n",
      "tensor([0.3248, 0.3321, 0.0000, 0.0005, 0.0010, 0.1369, 0.0000, 0.0005, 0.0000,\n",
      "        0.1159, 0.0000, 0.0000, 0.0168, 0.0005, 0.0000, 0.0220, 0.0000, 0.0000,\n",
      "        0.0000, 0.0037, 0.0010, 0.0010, 0.0000, 0.0000, 0.0000, 0.0425, 0.0005])\n",
      "tensor([619, 633,   0,   1,   2, 261,   0,   1,   0, 221,   0,   0,  32,   1,\n",
      "          0,  42,   0,   0,   0,   7,   2,   2,   0,   0,   0,  81,   1],\n",
      "       dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# looking at some of hte P distributions of certain bigrams coming up in the names\n",
    "print(stoi)\n",
    "prob = whole_set.get_P_bigrams('n','n')\n",
    "count = whole_set.get_N_bigrams('n','n')\n",
    "print(prob)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_Trigrams:\n",
    "    def __init__(self, name_list):\n",
    "        self.xs, self.ys = self.NN_dataset(name_list)\n",
    "        self.W, self.xenc = self.initialize_parameters()\n",
    "        self.gd_cycles = 0\n",
    "\n",
    "\n",
    "    def NN_dataset(self, names):\n",
    "        xs, ys = [], []\n",
    "        for name in names:\n",
    "            # append bigram to xs, result to ys\n",
    "            name = ['.', '.'] + list(name) + ['.']\n",
    "            for i in range(len(name)-2):\n",
    "                xs.append((stoi[name[i]], stoi[name[i+1]]))\n",
    "                ys.append(stoi[name[i+2]])\n",
    "        xs = torch.LongTensor(xs)           #tensor only holds values as ints\n",
    "        ys = torch.IntTensor(ys)\n",
    "        return xs, ys\n",
    "    \n",
    "    # defining NN parameters\n",
    "    def initialize_parameters(self):\n",
    "        W = torch.randn((54,27), requires_grad=True)\n",
    "        xenc = F.one_hot(self.xs, num_classes=27).float()\n",
    "        return W, xenc\n",
    "    \n",
    "    def grad_descent(self, cycles, step_size=0.05, print_losses=True):\n",
    "        # grad descent\n",
    "        xenc_reshaped = self.xenc.view(-1,54)\n",
    "        for i in range(cycles):\n",
    "            # forward pass\n",
    "            logits = xenc_reshaped @ self.W\n",
    "            counts = logits.exp()\n",
    "            p = counts / counts.sum(1, keepdim=True)\n",
    "            self.loss = -p[torch.arange(len(self.ys)), self.ys].log().mean()\n",
    "\n",
    "            # backward pass\n",
    "            self.W.grad = None\n",
    "            self.loss.backward()\n",
    "\n",
    "            # update params\n",
    "            self.W.data += -step_size * self.W.grad             #increased step size from 0.1 to -.2\n",
    "            self.gd_cycles += 1\n",
    "            if print_losses:\n",
    "                print('Cycle #{0}, loss = {1}'.format(self.gd_cycles, self.loss))\n",
    "\n",
    "    def get_names(self, num_names):\n",
    "        names = []\n",
    "        for i in range(num_names):\n",
    "            out = []\n",
    "            iix = ix = 0\n",
    "            while True:\n",
    "                iixenc = F.one_hot(torch.LongTensor([iix]), num_classes = 27).float()\n",
    "                ixenc = F.one_hot(torch.LongTensor([ix]), num_classes = 27).float()\n",
    "                xenc = torch.cat((iixenc, ixenc), -1)\n",
    "                logits = xenc @ self.W\n",
    "                counts = logits.exp()\n",
    "                p = counts / counts.sum(1, keepdim=True)\n",
    "                x = torch.multinomial(p, num_samples= 1, replacement=True).item()\n",
    "                # print(itos[iix], itos[ix], itos[x], p[0,x].data)\n",
    "                out.append(itos[x])\n",
    "                if x == 0:\n",
    "                    break\n",
    "                iix, ix = ix, x\n",
    "            names.append(''.join(out[:-1]))\n",
    "        return names\n",
    "    \n",
    "    def get_bigram_CP(self, ch1, ch2):\n",
    "        # iixenc = F.one_hot(torch.LongTensor([stoi[ch1]]), num_classes = 27).float()\n",
    "        # ixenc = F.one_hot(torch.LongTensor([stoi[ch2]]), num_classes = 27).float()\n",
    "        ix = torch.LongTensor([stoi[ch1], stoi[ch2]])\n",
    "        xenc = F.one_hot(ix, num_classes=27).float()\n",
    "        xenc_reshaped = xenc.view(-1,54)\n",
    "        logits = xenc_reshaped @ self.W\n",
    "        counts = logits.exp()\n",
    "        p = counts / counts.sum(1, keepdim=True)\n",
    "        return counts, p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cycle #1, loss = 3.971647262573242\n"
     ]
    }
   ],
   "source": [
    "full_list_nn = NN_Trigrams(names)\n",
    "full_list_nn.grad_descent(1, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cycle #502, loss = 3.0436205863952637\n",
      "Cycle #503, loss = 3.042783737182617\n",
      "Cycle #504, loss = 3.0419490337371826\n",
      "Cycle #505, loss = 3.041116237640381\n",
      "Cycle #506, loss = 3.040285348892212\n",
      "Cycle #507, loss = 3.0394561290740967\n",
      "Cycle #508, loss = 3.0386290550231934\n",
      "Cycle #509, loss = 3.0378036499023438\n",
      "Cycle #510, loss = 3.036980390548706\n",
      "Cycle #511, loss = 3.036159038543701\n",
      "Cycle #512, loss = 3.035339593887329\n",
      "Cycle #513, loss = 3.03452205657959\n",
      "Cycle #514, loss = 3.033705949783325\n",
      "Cycle #515, loss = 3.0328919887542725\n",
      "Cycle #516, loss = 3.0320804119110107\n",
      "Cycle #517, loss = 3.0312700271606445\n",
      "Cycle #518, loss = 3.030461311340332\n",
      "Cycle #519, loss = 3.0296549797058105\n",
      "Cycle #520, loss = 3.028850555419922\n",
      "Cycle #521, loss = 3.028047561645508\n",
      "Cycle #522, loss = 3.0272467136383057\n",
      "Cycle #523, loss = 3.0264477729797363\n",
      "Cycle #524, loss = 3.0256502628326416\n",
      "Cycle #525, loss = 3.0248546600341797\n",
      "Cycle #526, loss = 3.024061441421509\n",
      "Cycle #527, loss = 3.0232694149017334\n",
      "Cycle #528, loss = 3.0224790573120117\n",
      "Cycle #529, loss = 3.021690845489502\n",
      "Cycle #530, loss = 3.020904541015625\n",
      "Cycle #531, loss = 3.0201199054718018\n",
      "Cycle #532, loss = 3.0193371772766113\n",
      "Cycle #533, loss = 3.0185561180114746\n",
      "Cycle #534, loss = 3.0177769660949707\n",
      "Cycle #535, loss = 3.0169990062713623\n",
      "Cycle #536, loss = 3.016223192214966\n",
      "Cycle #537, loss = 3.015449285507202\n",
      "Cycle #538, loss = 3.014677047729492\n",
      "Cycle #539, loss = 3.013906717300415\n",
      "Cycle #540, loss = 3.0131380558013916\n",
      "Cycle #541, loss = 3.0123705863952637\n",
      "Cycle #542, loss = 3.011605739593506\n",
      "Cycle #543, loss = 3.0108420848846436\n",
      "Cycle #544, loss = 3.010080099105835\n",
      "Cycle #545, loss = 3.00931978225708\n",
      "Cycle #546, loss = 3.008561611175537\n",
      "Cycle #547, loss = 3.0078048706054688\n",
      "Cycle #548, loss = 3.007050037384033\n",
      "Cycle #549, loss = 3.0062966346740723\n",
      "Cycle #550, loss = 3.005545139312744\n",
      "Cycle #551, loss = 3.0047950744628906\n",
      "Cycle #552, loss = 3.004046678543091\n",
      "Cycle #553, loss = 3.003300905227661\n",
      "Cycle #554, loss = 3.0025558471679688\n",
      "Cycle #555, loss = 3.001812696456909\n",
      "Cycle #556, loss = 3.0010712146759033\n",
      "Cycle #557, loss = 3.000331163406372\n",
      "Cycle #558, loss = 2.9995932579040527\n",
      "Cycle #559, loss = 2.998856544494629\n",
      "Cycle #560, loss = 2.998121976852417\n",
      "Cycle #561, loss = 2.9973888397216797\n",
      "Cycle #562, loss = 2.996657133102417\n",
      "Cycle #563, loss = 2.995927333831787\n",
      "Cycle #564, loss = 2.99519944190979\n",
      "Cycle #565, loss = 2.9944722652435303\n",
      "Cycle #566, loss = 2.9937472343444824\n",
      "Cycle #567, loss = 2.993023633956909\n",
      "Cycle #568, loss = 2.992302417755127\n",
      "Cycle #569, loss = 2.991582155227661\n",
      "Cycle #570, loss = 2.990863561630249\n",
      "Cycle #571, loss = 2.9901463985443115\n",
      "Cycle #572, loss = 2.9894309043884277\n",
      "Cycle #573, loss = 2.9887173175811768\n",
      "Cycle #574, loss = 2.9880049228668213\n",
      "Cycle #575, loss = 2.9872944355010986\n",
      "Cycle #576, loss = 2.9865853786468506\n",
      "Cycle #577, loss = 2.9858779907226562\n",
      "Cycle #578, loss = 2.9851722717285156\n",
      "Cycle #579, loss = 2.9844677448272705\n",
      "Cycle #580, loss = 2.983764886856079\n",
      "Cycle #581, loss = 2.9830634593963623\n",
      "Cycle #582, loss = 2.9823641777038574\n",
      "Cycle #583, loss = 2.981665849685669\n",
      "Cycle #584, loss = 2.9809694290161133\n",
      "Cycle #585, loss = 2.980274200439453\n",
      "Cycle #586, loss = 2.979580879211426\n",
      "Cycle #587, loss = 2.978888750076294\n",
      "Cycle #588, loss = 2.978198528289795\n",
      "Cycle #589, loss = 2.9775094985961914\n",
      "Cycle #590, loss = 2.9768223762512207\n",
      "Cycle #591, loss = 2.9761364459991455\n",
      "Cycle #592, loss = 2.975452184677124\n",
      "Cycle #593, loss = 2.974769115447998\n",
      "Cycle #594, loss = 2.974087953567505\n",
      "Cycle #595, loss = 2.9734079837799072\n",
      "Cycle #596, loss = 2.9727296829223633\n",
      "Cycle #597, loss = 2.972052812576294\n",
      "Cycle #598, loss = 2.97137713432312\n",
      "Cycle #599, loss = 2.970703601837158\n",
      "Cycle #600, loss = 2.9700307846069336\n",
      "Cycle #601, loss = 2.969360113143921\n",
      "Cycle #602, loss = 2.9686906337738037\n",
      "Cycle #603, loss = 2.968022346496582\n",
      "Cycle #604, loss = 2.967355489730835\n",
      "Cycle #605, loss = 2.9666907787323\n",
      "Cycle #606, loss = 2.966027021408081\n",
      "Cycle #607, loss = 2.965364933013916\n",
      "Cycle #608, loss = 2.9647040367126465\n",
      "Cycle #609, loss = 2.9640445709228516\n",
      "Cycle #610, loss = 2.9633867740631104\n",
      "Cycle #611, loss = 2.9627301692962646\n",
      "Cycle #612, loss = 2.9620752334594727\n",
      "Cycle #613, loss = 2.9614217281341553\n",
      "Cycle #614, loss = 2.9607694149017334\n",
      "Cycle #615, loss = 2.9601187705993652\n",
      "Cycle #616, loss = 2.9594693183898926\n",
      "Cycle #617, loss = 2.9588212966918945\n",
      "Cycle #618, loss = 2.958174467086792\n",
      "Cycle #619, loss = 2.9575295448303223\n",
      "Cycle #620, loss = 2.956885814666748\n",
      "Cycle #621, loss = 2.9562435150146484\n",
      "Cycle #622, loss = 2.9556024074554443\n",
      "Cycle #623, loss = 2.954962730407715\n",
      "Cycle #624, loss = 2.954324722290039\n",
      "Cycle #625, loss = 2.953688144683838\n",
      "Cycle #626, loss = 2.953052282333374\n",
      "Cycle #627, loss = 2.952418088912964\n",
      "Cycle #628, loss = 2.9517853260040283\n",
      "Cycle #629, loss = 2.9511539936065674\n",
      "Cycle #630, loss = 2.950524091720581\n",
      "Cycle #631, loss = 2.9498953819274902\n",
      "Cycle #632, loss = 2.949268341064453\n",
      "Cycle #633, loss = 2.9486420154571533\n",
      "Cycle #634, loss = 2.9480173587799072\n",
      "Cycle #635, loss = 2.9473938941955566\n",
      "Cycle #636, loss = 2.946772336959839\n",
      "Cycle #637, loss = 2.9461512565612793\n",
      "Cycle #638, loss = 2.9455320835113525\n",
      "Cycle #639, loss = 2.9449141025543213\n",
      "Cycle #640, loss = 2.9442975521087646\n",
      "Cycle #641, loss = 2.9436817169189453\n",
      "Cycle #642, loss = 2.943068027496338\n",
      "Cycle #643, loss = 2.942455530166626\n",
      "Cycle #644, loss = 2.9418439865112305\n",
      "Cycle #645, loss = 2.9412338733673096\n",
      "Cycle #646, loss = 2.9406251907348633\n",
      "Cycle #647, loss = 2.9400172233581543\n",
      "Cycle #648, loss = 2.939411163330078\n",
      "Cycle #649, loss = 2.9388062953948975\n",
      "Cycle #650, loss = 2.938202381134033\n",
      "Cycle #651, loss = 2.9375998973846436\n",
      "Cycle #652, loss = 2.9369986057281494\n",
      "Cycle #653, loss = 2.936398983001709\n",
      "Cycle #654, loss = 2.935800313949585\n",
      "Cycle #655, loss = 2.9352028369903564\n",
      "Cycle #656, loss = 2.9346070289611816\n",
      "Cycle #657, loss = 2.9340121746063232\n",
      "Cycle #658, loss = 2.9334187507629395\n",
      "Cycle #659, loss = 2.932826519012451\n",
      "Cycle #660, loss = 2.9322352409362793\n",
      "Cycle #661, loss = 2.931645393371582\n",
      "Cycle #662, loss = 2.9310567378997803\n",
      "Cycle #663, loss = 2.9304697513580322\n",
      "Cycle #664, loss = 2.9298832416534424\n",
      "Cycle #665, loss = 2.9292986392974854\n",
      "Cycle #666, loss = 2.9287147521972656\n",
      "Cycle #667, loss = 2.9281320571899414\n",
      "Cycle #668, loss = 2.927551031112671\n",
      "Cycle #669, loss = 2.9269707202911377\n",
      "Cycle #670, loss = 2.9263923168182373\n",
      "Cycle #671, loss = 2.9258148670196533\n",
      "Cycle #672, loss = 2.9252378940582275\n",
      "Cycle #673, loss = 2.9246628284454346\n",
      "Cycle #674, loss = 2.924088954925537\n",
      "Cycle #675, loss = 2.923516035079956\n",
      "Cycle #676, loss = 2.9229445457458496\n",
      "Cycle #677, loss = 2.9223742485046387\n",
      "Cycle #678, loss = 2.921804666519165\n",
      "Cycle #679, loss = 2.921236753463745\n",
      "Cycle #680, loss = 2.9206697940826416\n",
      "Cycle #681, loss = 2.9201037883758545\n",
      "Cycle #682, loss = 2.919539451599121\n",
      "Cycle #683, loss = 2.918976068496704\n",
      "Cycle #684, loss = 2.9184138774871826\n",
      "Cycle #685, loss = 2.9178528785705566\n",
      "Cycle #686, loss = 2.917292833328247\n",
      "Cycle #687, loss = 2.916733980178833\n",
      "Cycle #688, loss = 2.9161765575408936\n",
      "Cycle #689, loss = 2.9156198501586914\n",
      "Cycle #690, loss = 2.915064811706543\n",
      "Cycle #691, loss = 2.914510726928711\n",
      "Cycle #692, loss = 2.9139575958251953\n",
      "Cycle #693, loss = 2.913405179977417\n",
      "Cycle #694, loss = 2.9128549098968506\n",
      "Cycle #695, loss = 2.9123051166534424\n",
      "Cycle #696, loss = 2.9117562770843506\n",
      "Cycle #697, loss = 2.9112091064453125\n",
      "Cycle #698, loss = 2.9106626510620117\n",
      "Cycle #699, loss = 2.9101173877716064\n",
      "Cycle #700, loss = 2.9095733165740967\n",
      "Cycle #701, loss = 2.9090306758880615\n",
      "Cycle #702, loss = 2.9084887504577637\n",
      "Cycle #703, loss = 2.9079480171203613\n",
      "Cycle #704, loss = 2.9074082374572754\n",
      "Cycle #705, loss = 2.906869649887085\n",
      "Cycle #706, loss = 2.90633225440979\n",
      "Cycle #707, loss = 2.9057955741882324\n",
      "Cycle #708, loss = 2.9052603244781494\n",
      "Cycle #709, loss = 2.904726028442383\n",
      "Cycle #710, loss = 2.904193162918091\n",
      "Cycle #711, loss = 2.903661012649536\n",
      "Cycle #712, loss = 2.903130054473877\n",
      "Cycle #713, loss = 2.902599811553955\n",
      "Cycle #714, loss = 2.902070999145508\n",
      "Cycle #715, loss = 2.901543617248535\n",
      "Cycle #716, loss = 2.9010162353515625\n",
      "Cycle #717, loss = 2.9004907608032227\n",
      "Cycle #718, loss = 2.899966239929199\n",
      "Cycle #719, loss = 2.899442672729492\n",
      "Cycle #720, loss = 2.8989202976226807\n",
      "Cycle #721, loss = 2.8983983993530273\n",
      "Cycle #722, loss = 2.8978779315948486\n",
      "Cycle #723, loss = 2.8973586559295654\n",
      "Cycle #724, loss = 2.8968400955200195\n",
      "Cycle #725, loss = 2.8963229656219482\n",
      "Cycle #726, loss = 2.8958067893981934\n",
      "Cycle #727, loss = 2.895291328430176\n",
      "Cycle #728, loss = 2.894777297973633\n",
      "Cycle #729, loss = 2.894263982772827\n",
      "Cycle #730, loss = 2.893751382827759\n",
      "Cycle #731, loss = 2.893240213394165\n",
      "Cycle #732, loss = 2.892730474472046\n",
      "Cycle #733, loss = 2.892221450805664\n",
      "Cycle #734, loss = 2.8917126655578613\n",
      "Cycle #735, loss = 2.8912055492401123\n",
      "Cycle #736, loss = 2.8906993865966797\n",
      "Cycle #737, loss = 2.8901941776275635\n",
      "Cycle #738, loss = 2.889690399169922\n",
      "Cycle #739, loss = 2.8891868591308594\n",
      "Cycle #740, loss = 2.8886847496032715\n",
      "Cycle #741, loss = 2.888183832168579\n",
      "Cycle #742, loss = 2.887683391571045\n",
      "Cycle #743, loss = 2.887183904647827\n",
      "Cycle #744, loss = 2.886685609817505\n",
      "Cycle #745, loss = 2.886188507080078\n",
      "Cycle #746, loss = 2.8856921195983887\n",
      "Cycle #747, loss = 2.8851969242095947\n",
      "Cycle #748, loss = 2.884702444076538\n",
      "Cycle #749, loss = 2.884208917617798\n",
      "Cycle #750, loss = 2.883716106414795\n",
      "Cycle #751, loss = 2.8832247257232666\n",
      "Cycle #752, loss = 2.8827342987060547\n",
      "Cycle #753, loss = 2.882244825363159\n",
      "Cycle #754, loss = 2.881756067276001\n",
      "Cycle #755, loss = 2.8812685012817383\n",
      "Cycle #756, loss = 2.880781650543213\n",
      "Cycle #757, loss = 2.880295991897583\n",
      "Cycle #758, loss = 2.8798110485076904\n",
      "Cycle #759, loss = 2.8793270587921143\n",
      "Cycle #760, loss = 2.8788442611694336\n",
      "Cycle #761, loss = 2.878361940383911\n",
      "Cycle #762, loss = 2.877880811691284\n",
      "Cycle #763, loss = 2.8774006366729736\n",
      "Cycle #764, loss = 2.8769214153289795\n",
      "Cycle #765, loss = 2.8764429092407227\n",
      "Cycle #766, loss = 2.875965118408203\n",
      "Cycle #767, loss = 2.8754889965057373\n",
      "Cycle #768, loss = 2.8750128746032715\n",
      "Cycle #769, loss = 2.8745384216308594\n",
      "Cycle #770, loss = 2.8740644454956055\n",
      "Cycle #771, loss = 2.873591661453247\n",
      "Cycle #772, loss = 2.873119592666626\n",
      "Cycle #773, loss = 2.8726484775543213\n",
      "Cycle #774, loss = 2.872178316116333\n",
      "Cycle #775, loss = 2.871709108352661\n",
      "Cycle #776, loss = 2.8712406158447266\n",
      "Cycle #777, loss = 2.8707730770111084\n",
      "Cycle #778, loss = 2.8703064918518066\n",
      "Cycle #779, loss = 2.869840383529663\n",
      "Cycle #780, loss = 2.869375467300415\n",
      "Cycle #781, loss = 2.8689115047454834\n",
      "Cycle #782, loss = 2.868448495864868\n",
      "Cycle #783, loss = 2.8679862022399902\n",
      "Cycle #784, loss = 2.867525100708008\n",
      "Cycle #785, loss = 2.8670642375946045\n",
      "Cycle #786, loss = 2.8666045665740967\n",
      "Cycle #787, loss = 2.8661458492279053\n",
      "Cycle #788, loss = 2.8656880855560303\n",
      "Cycle #789, loss = 2.8652307987213135\n",
      "Cycle #790, loss = 2.864774703979492\n",
      "Cycle #791, loss = 2.864319086074829\n",
      "Cycle #792, loss = 2.8638646602630615\n",
      "Cycle #793, loss = 2.8634109497070312\n",
      "Cycle #794, loss = 2.8629584312438965\n",
      "Cycle #795, loss = 2.86250638961792\n",
      "Cycle #796, loss = 2.8620550632476807\n",
      "Cycle #797, loss = 2.861604690551758\n",
      "Cycle #798, loss = 2.8611555099487305\n",
      "Cycle #799, loss = 2.8607070446014404\n",
      "Cycle #800, loss = 2.8602588176727295\n",
      "Cycle #801, loss = 2.859811782836914\n",
      "Cycle #802, loss = 2.859365940093994\n",
      "Cycle #803, loss = 2.8589205741882324\n",
      "Cycle #804, loss = 2.858475923538208\n",
      "Cycle #805, loss = 2.8580322265625\n",
      "Cycle #806, loss = 2.8575892448425293\n",
      "Cycle #807, loss = 2.857147455215454\n",
      "Cycle #808, loss = 2.856706142425537\n",
      "Cycle #809, loss = 2.8562657833099365\n",
      "Cycle #810, loss = 2.8558261394500732\n",
      "Cycle #811, loss = 2.8553874492645264\n",
      "Cycle #812, loss = 2.8549492359161377\n",
      "Cycle #813, loss = 2.8545119762420654\n",
      "Cycle #814, loss = 2.8540759086608887\n",
      "Cycle #815, loss = 2.85364031791687\n",
      "Cycle #816, loss = 2.8532052040100098\n",
      "Cycle #817, loss = 2.852771282196045\n",
      "Cycle #818, loss = 2.8523383140563965\n",
      "Cycle #819, loss = 2.851905584335327\n",
      "Cycle #820, loss = 2.8514740467071533\n",
      "Cycle #821, loss = 2.8510429859161377\n",
      "Cycle #822, loss = 2.8506128787994385\n",
      "Cycle #823, loss = 2.8501839637756348\n",
      "Cycle #824, loss = 2.849754810333252\n",
      "Cycle #825, loss = 2.849327564239502\n",
      "Cycle #826, loss = 2.848900556564331\n",
      "Cycle #827, loss = 2.8484745025634766\n",
      "Cycle #828, loss = 2.848048686981201\n",
      "Cycle #829, loss = 2.8476245403289795\n",
      "Cycle #830, loss = 2.847200393676758\n",
      "Cycle #831, loss = 2.8467772006988525\n",
      "Cycle #832, loss = 2.8463547229766846\n",
      "Cycle #833, loss = 2.845933198928833\n",
      "Cycle #834, loss = 2.8455119132995605\n",
      "Cycle #835, loss = 2.8450920581817627\n",
      "Cycle #836, loss = 2.844672441482544\n",
      "Cycle #837, loss = 2.8442540168762207\n",
      "Cycle #838, loss = 2.8438360691070557\n",
      "Cycle #839, loss = 2.843418836593628\n",
      "Cycle #840, loss = 2.8430025577545166\n",
      "Cycle #841, loss = 2.8425867557525635\n",
      "Cycle #842, loss = 2.842172145843506\n",
      "Cycle #843, loss = 2.8417575359344482\n",
      "Cycle #844, loss = 2.841344118118286\n",
      "Cycle #845, loss = 2.8409314155578613\n",
      "Cycle #846, loss = 2.840519666671753\n",
      "Cycle #847, loss = 2.8401083946228027\n",
      "Cycle #848, loss = 2.83969783782959\n",
      "Cycle #849, loss = 2.8392879962921143\n",
      "Cycle #850, loss = 2.838878631591797\n",
      "Cycle #851, loss = 2.838470697402954\n",
      "Cycle #852, loss = 2.8380627632141113\n",
      "Cycle #853, loss = 2.837656021118164\n",
      "Cycle #854, loss = 2.837249517440796\n",
      "Cycle #855, loss = 2.836843967437744\n",
      "Cycle #856, loss = 2.836439371109009\n",
      "Cycle #857, loss = 2.8360352516174316\n",
      "Cycle #858, loss = 2.835631847381592\n",
      "Cycle #859, loss = 2.8352291584014893\n",
      "Cycle #860, loss = 2.834827184677124\n",
      "Cycle #861, loss = 2.834425926208496\n",
      "Cycle #862, loss = 2.8340253829956055\n",
      "Cycle #863, loss = 2.8336257934570312\n",
      "Cycle #864, loss = 2.833226203918457\n",
      "Cycle #865, loss = 2.8328278064727783\n",
      "Cycle #866, loss = 2.8324296474456787\n",
      "Cycle #867, loss = 2.8320326805114746\n",
      "Cycle #868, loss = 2.831636667251587\n",
      "Cycle #869, loss = 2.831240653991699\n",
      "Cycle #870, loss = 2.830845355987549\n",
      "Cycle #871, loss = 2.830451011657715\n",
      "Cycle #872, loss = 2.830057382583618\n",
      "Cycle #873, loss = 2.8296642303466797\n",
      "Cycle #874, loss = 2.8292717933654785\n",
      "Cycle #875, loss = 2.8288800716400146\n",
      "Cycle #876, loss = 2.828488826751709\n",
      "Cycle #877, loss = 2.828098773956299\n",
      "Cycle #878, loss = 2.8277089595794678\n",
      "Cycle #879, loss = 2.8273203372955322\n",
      "Cycle #880, loss = 2.826932191848755\n",
      "Cycle #881, loss = 2.8265442848205566\n",
      "Cycle #882, loss = 2.8261570930480957\n",
      "Cycle #883, loss = 2.825770854949951\n",
      "Cycle #884, loss = 2.8253848552703857\n",
      "Cycle #885, loss = 2.825000047683716\n",
      "Cycle #886, loss = 2.824615478515625\n",
      "Cycle #887, loss = 2.8242318630218506\n",
      "Cycle #888, loss = 2.8238484859466553\n",
      "Cycle #889, loss = 2.8234663009643555\n",
      "Cycle #890, loss = 2.8230841159820557\n",
      "Cycle #891, loss = 2.8227031230926514\n",
      "Cycle #892, loss = 2.8223226070404053\n",
      "Cycle #893, loss = 2.8219423294067383\n",
      "Cycle #894, loss = 2.821563482284546\n",
      "Cycle #895, loss = 2.8211846351623535\n",
      "Cycle #896, loss = 2.8208067417144775\n",
      "Cycle #897, loss = 2.8204293251037598\n",
      "Cycle #898, loss = 2.8200523853302\n",
      "Cycle #899, loss = 2.8196768760681152\n",
      "Cycle #900, loss = 2.819301128387451\n",
      "Cycle #901, loss = 2.8189260959625244\n",
      "Cycle #902, loss = 2.818552017211914\n",
      "Cycle #903, loss = 2.818178415298462\n",
      "Cycle #904, loss = 2.817805528640747\n",
      "Cycle #905, loss = 2.8174328804016113\n",
      "Cycle #906, loss = 2.817061185836792\n",
      "Cycle #907, loss = 2.81669020652771\n",
      "Cycle #908, loss = 2.816319704055786\n",
      "Cycle #909, loss = 2.8159501552581787\n",
      "Cycle #910, loss = 2.8155806064605713\n",
      "Cycle #911, loss = 2.815211772918701\n",
      "Cycle #912, loss = 2.8148441314697266\n",
      "Cycle #913, loss = 2.814476251602173\n",
      "Cycle #914, loss = 2.8141090869903564\n",
      "Cycle #915, loss = 2.8137433528900146\n",
      "Cycle #916, loss = 2.8133773803710938\n",
      "Cycle #917, loss = 2.8130125999450684\n",
      "Cycle #918, loss = 2.812647819519043\n",
      "Cycle #919, loss = 2.812284469604492\n",
      "Cycle #920, loss = 2.8119211196899414\n",
      "Cycle #921, loss = 2.811558246612549\n",
      "Cycle #922, loss = 2.8111960887908936\n",
      "Cycle #923, loss = 2.8108348846435547\n",
      "Cycle #924, loss = 2.810473918914795\n",
      "Cycle #925, loss = 2.8101136684417725\n",
      "Cycle #926, loss = 2.809753894805908\n",
      "Cycle #927, loss = 2.8093950748443604\n",
      "Cycle #928, loss = 2.8090362548828125\n",
      "Cycle #929, loss = 2.808678150177002\n",
      "Cycle #930, loss = 2.8083207607269287\n",
      "Cycle #931, loss = 2.8079640865325928\n",
      "Cycle #932, loss = 2.807607889175415\n",
      "Cycle #933, loss = 2.8072516918182373\n",
      "Cycle #934, loss = 2.806896686553955\n",
      "Cycle #935, loss = 2.806542158126831\n",
      "Cycle #936, loss = 2.8061883449554443\n",
      "Cycle #937, loss = 2.8058347702026367\n",
      "Cycle #938, loss = 2.8054823875427246\n",
      "Cycle #939, loss = 2.8051297664642334\n",
      "Cycle #940, loss = 2.8047783374786377\n",
      "Cycle #941, loss = 2.804426670074463\n",
      "Cycle #942, loss = 2.8040764331817627\n",
      "Cycle #943, loss = 2.8037266731262207\n",
      "Cycle #944, loss = 2.8033769130706787\n",
      "Cycle #945, loss = 2.803028106689453\n",
      "Cycle #946, loss = 2.8026795387268066\n",
      "Cycle #947, loss = 2.8023321628570557\n",
      "Cycle #948, loss = 2.8019845485687256\n",
      "Cycle #949, loss = 2.80163836479187\n",
      "Cycle #950, loss = 2.8012917041778564\n",
      "Cycle #951, loss = 2.800945997238159\n",
      "Cycle #952, loss = 2.8006014823913574\n",
      "Cycle #953, loss = 2.8002567291259766\n",
      "Cycle #954, loss = 2.799912691116333\n",
      "Cycle #955, loss = 2.7995693683624268\n",
      "Cycle #956, loss = 2.799226760864258\n",
      "Cycle #957, loss = 2.798884153366089\n",
      "Cycle #958, loss = 2.7985424995422363\n",
      "Cycle #959, loss = 2.798201084136963\n",
      "Cycle #960, loss = 2.797860622406006\n",
      "Cycle #961, loss = 2.797520637512207\n",
      "Cycle #962, loss = 2.797180652618408\n",
      "Cycle #963, loss = 2.7968413829803467\n",
      "Cycle #964, loss = 2.7965028285980225\n",
      "Cycle #965, loss = 2.7961645126342773\n",
      "Cycle #966, loss = 2.7958273887634277\n",
      "Cycle #967, loss = 2.795490026473999\n",
      "Cycle #968, loss = 2.7951536178588867\n",
      "Cycle #969, loss = 2.7948176860809326\n",
      "Cycle #970, loss = 2.794482469558716\n",
      "Cycle #971, loss = 2.794147491455078\n",
      "Cycle #972, loss = 2.7938129901885986\n",
      "Cycle #973, loss = 2.7934789657592773\n",
      "Cycle #974, loss = 2.7931456565856934\n",
      "Cycle #975, loss = 2.7928125858306885\n",
      "Cycle #976, loss = 2.79248046875\n",
      "Cycle #977, loss = 2.7921481132507324\n",
      "Cycle #978, loss = 2.7918174266815186\n",
      "Cycle #979, loss = 2.7914862632751465\n",
      "Cycle #980, loss = 2.79115629196167\n",
      "Cycle #981, loss = 2.7908263206481934\n",
      "Cycle #982, loss = 2.790496826171875\n",
      "Cycle #983, loss = 2.790168046951294\n",
      "Cycle #984, loss = 2.789839506149292\n",
      "Cycle #985, loss = 2.7895119190216064\n",
      "Cycle #986, loss = 2.7891845703125\n",
      "Cycle #987, loss = 2.78885817527771\n",
      "Cycle #988, loss = 2.78853178024292\n",
      "Cycle #989, loss = 2.788205862045288\n",
      "Cycle #990, loss = 2.7878804206848145\n",
      "Cycle #991, loss = 2.787555694580078\n",
      "Cycle #992, loss = 2.7872314453125\n",
      "Cycle #993, loss = 2.786907196044922\n",
      "Cycle #994, loss = 2.7865843772888184\n",
      "Cycle #995, loss = 2.7862613201141357\n",
      "Cycle #996, loss = 2.7859389781951904\n",
      "Cycle #997, loss = 2.7856171131134033\n",
      "Cycle #998, loss = 2.7852957248687744\n",
      "Cycle #999, loss = 2.7849748134613037\n",
      "Cycle #1000, loss = 2.784654378890991\n",
      "Cycle #1001, loss = 2.784334421157837\n"
     ]
    }
   ],
   "source": [
    "full_list_nn.grad_descent(500, 0.25, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['swor', 'mafzhgzie', 'jcnbzoen', 'aon', 'san', 'jjtykem', 'jsza', 'renxiw', 'ajtxnqsellyaucn', 'eh']\n"
     ]
    }
   ],
   "source": [
    "trigram_names_NN = full_list_nn.get_names(10)\n",
    "print(trigram_names_NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '.'), (1, 'a'), (2, 'b'), (3, 'c'), (4, 'd'), (5, 'e'), (6, 'f'), (7, 'g'), (8, 'h')]\n",
      "[(9, 'i'), (10, 'j'), (11, 'k'), (12, 'l'), (13, 'm'), (14, 'n'), (15, 'o'), (16, 'p'), (17, 'q')]\n",
      "[(18, 'r'), (19, 's'), (20, 't'), (21, 'u'), (22, 'v'), (23, 'w'), (24, 'x'), (25, 'y'), (26, 'z')]\n",
      "tensor([[0.7665, 1.2685, 0.7532, 0.6770, 0.0859, 0.1488, 0.3046, 2.3514, 1.3209,\n",
      "         1.5249, 1.8975, 0.3693, 0.1325, 1.2437, 0.5347, 0.6147, 0.0628, 0.2194,\n",
      "         1.2475, 0.2340, 8.2969, 0.7160, 1.2995, 1.2223, 2.6417, 1.6705, 0.9754]],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "tensor([[0.0235, 0.0389, 0.0231, 0.0208, 0.0026, 0.0046, 0.0093, 0.0722, 0.0405,\n",
      "         0.0468, 0.0582, 0.0113, 0.0041, 0.0382, 0.0164, 0.0189, 0.0019, 0.0067,\n",
      "         0.0383, 0.0072, 0.2547, 0.0220, 0.0399, 0.0375, 0.0811, 0.0513, 0.0299]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "lett = sorted(itos.items())\n",
    "for i in range(3):\n",
    "    print(lett[i*9:(i+1)*9])\n",
    "    \n",
    "x_count, x_prob = full_list_nn.get_bigram_CP('j','j')\n",
    "print(x_count)\n",
    "print(x_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E02: split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25626 3203 3204\n"
     ]
    }
   ],
   "source": [
    "# split dataset into 80:10:10 - train 2 and 3gram on training set - evaluate on dev and test\n",
    "import random\n",
    "random.shuffle(names)\n",
    "num_names = len(names)\n",
    "training_set = names[:int(0.8*num_names)]\n",
    "dev_set = names[int(0.8*num_names):int(0.9*num_names)]\n",
    "test_set = names[int(0.9*num_names):]\n",
    "print(len(training_set), len(dev_set), len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E03: use the dev set to tune the strength of smoothing (or regularization) for the trigram model - i.e. try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E04: we saw that our 1-hot vectors merely select a row of W, so producing these vectors explicitly feels wasteful. Can you delete our use of F.one_hot in favor of simply indexing into rows of W?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E05: look up and use F.cross_entropy instead. You should achieve the same result. Can you think of why we'd prefer to use F.cross_entropy instead?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E06: meta-exercise! Think of a fun/interesting exercise and complete it.\n",
    "\n",
    "\n",
    "Useful links for practice:\n",
    "- Python + Numpy tutorial from CS231n https://cs231n.github.io/python-numpy... . We use torch.tensor instead of numpy.array in this video. Their design (e.g. broadcasting, data types, etc.) is so similar that practicing one is basically practicing the other, just be careful with some of the APIs - how various functions are named, what arguments they take, etc. - these details can vary.\n",
    "- PyTorch tutorial on Tensor https://pytorch.org/tutorials/beginne...\n",
    "- Another PyTorch intro to Tensor https://pytorch.org/tutorials/beginne...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
