{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigram Homework\n",
    "\n",
    "Exercises:\n",
    "E01: train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F \n",
    "\n",
    "names = open('names.txt', 'r').read().splitlines()\n",
    "stoi = {ch:i+1 for i, ch in enumerate(sorted(list(set(''.join(names)))))}\n",
    "stoi['.'] = 0\n",
    "itos = {i:ch for ch, i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting method\n",
    "# make dataset\n",
    "class Counting:\n",
    "    def __init__(self, name_list):\n",
    "        self.name_list = name_list\n",
    "        self.P = self.trigram_counting_p_dist()\n",
    "        self.loss = self.counting_p_loss()\n",
    "\n",
    "    def trigram_counting_p_dist(self):\n",
    "        N = torch.zeros((27,27,27), dtype = torch.int32)\n",
    "        for name in self.name_list:\n",
    "            name = ['.', '.'] + list(name) + ['.']\n",
    "            for i in range(len(name) - 2):\n",
    "                N[stoi[name[i]], stoi[name[i+1]], stoi[name[i+2]]] += 1\n",
    "        P = (N+1).float()\n",
    "        P /= P.sum(2, keepdim=True)         #this was hard.... tried 1, and omitting keepdim\n",
    "        return P\n",
    "\n",
    "    def counting_p_loss(self):\n",
    "        log_likelihood = 0\n",
    "        num_trigrams = 0\n",
    "        for name in self.name_list:\n",
    "            name = ['.', '.'] + list(name) + ['.']\n",
    "            for i in range(len(name) - 2):\n",
    "                log_likelihood += self.P[stoi[name[i]], stoi[name[i+1]], stoi[name[i+2]]].log()\n",
    "                num_trigrams += 1\n",
    "        loss = - log_likelihood/num_trigrams\n",
    "        return loss\n",
    "\n",
    "    def get_names(self,num_names):\n",
    "        names = []\n",
    "        for i in range(num_names):\n",
    "            iix = ix = 0\n",
    "            out = []\n",
    "            while True:\n",
    "                p = self.P[iix, ix]       # same result, this works\n",
    "                x = torch.multinomial(p, num_samples=1, replacement=True).item()\n",
    "                out.append(itos[x])\n",
    "                if x == 0:\n",
    "                    break\n",
    "                iix, ix = ix, x\n",
    "            names.append(''.join(out[:-1]))\n",
    "        return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2120)\n",
      "['mee', 'athelin', 'lyn', 'bramarsdennai', 'vi', 'pozea', 'zymeritall', 'kmxbderdo', 'tavion', 'maday']\n"
     ]
    }
   ],
   "source": [
    "whole_set = Counting(names)\n",
    "print(whole_set.loss)\n",
    "some_names = whole_set.get_names(10)\n",
    "print(some_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, '.': 0}\n",
      "tensor(980)\n",
      "tensor([ 12,  12,  12, 115,  12,  12,  12,  25,  38,  12,  12, 474,  51,  12,\n",
      "         12,  25,  12,  12,  12,  12,  12,  12,  12,  12,  12,  12,  12],\n",
      "       dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# looking at some of hte P distributions of certain bigrams coming up in the names\n",
    "\n",
    "N = torch.zeros((27,27,27), dtype = torch.int32)\n",
    "print(stoi)\n",
    "for name in names:\n",
    "    name = ['.', '.'] + list(name) + ['.']\n",
    "    for i in range(len(name) - 2):\n",
    "        N[stoi[name[i]], stoi[name[i+1]], stoi[name[i+2]]] += 1\n",
    "P = (N+1).float()\n",
    "P /= P.sum(2, keepdim=True)         #this was hard.... tried 1, and omitting keepdim\n",
    "x = (P[13,3]*1000).int()\n",
    "print(x.sum())\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trigram_NN:\n",
    "    def __init__(self, name_list):\n",
    "        self.xs, self.ys = self.NN_dataset(name_list)\n",
    "        self.W, self.xenc = self.initialize_parameters()\n",
    "\n",
    "\n",
    "    def NN_dataset(self, names):\n",
    "        xs, ys = [], []\n",
    "        for name in names:\n",
    "            # append bigram to xs, result to ys\n",
    "            name = ['.', '.'] + list(name) + ['.']\n",
    "            for i in range(len(name)-2):\n",
    "                xs.append((stoi[name[i]], stoi[name[i+1]]))\n",
    "                ys.append(stoi[name[i+2]])\n",
    "        xs = torch.LongTensor(xs)           #tensor only holds values as ints\n",
    "        ys = torch.IntTensor(ys)\n",
    "        return xs, ys\n",
    "    \n",
    "    # defining NN parameters\n",
    "    def initialize_parameters(self):\n",
    "        W = torch.randn((54,27), requires_grad=True)\n",
    "        xenc = F.one_hot(self.xs, num_classes=27).float()\n",
    "        return W, xenc\n",
    "    \n",
    "    def grad_descent(self, cycles, step_size=0.05, print_losses=True):\n",
    "        # grad descent\n",
    "        xenc_reshaped = self.xenc.view(-1,54)\n",
    "        for i in range(cycles):\n",
    "            # forward pass\n",
    "            logits = xenc_reshaped @ self.W\n",
    "            counts = logits.exp()\n",
    "            p = counts / counts.sum(1, keepdim=True)\n",
    "            self.loss = -p[torch.arange(len(self.ys)), self.ys].log().mean()\n",
    "\n",
    "            # backward pass\n",
    "            self.W.grad = None\n",
    "            self.loss.backward()\n",
    "\n",
    "            # update params\n",
    "            self.W.data += -step_size * self.W.grad             #increased step size from 0.1 to -.2\n",
    "            if print_losses:\n",
    "                print('Cycle #{0}, loss = {1}'.format(i+1, self.loss))\n",
    "\n",
    "    def get_names(self, num_names):\n",
    "        names = []\n",
    "        for i in range(num_names):\n",
    "            out = []\n",
    "            iix = ix = 0\n",
    "            while True:\n",
    "                iixenc = F.one_hot(torch.LongTensor([iix]), num_classes = 27).float()\n",
    "                ixenc = F.one_hot(torch.LongTensor([ix]), num_classes = 27).float()\n",
    "                xenc = torch.cat((iixenc, ixenc), -1)\n",
    "                logits = xenc @ self.W\n",
    "                counts = logits.exp()\n",
    "                p = counts / counts.sum(1, keepdim=True)\n",
    "                x = torch.multinomial(p, num_samples= 1, replacement=True).item()\n",
    "                # print(itos[iix], itos[ix], itos[x], p[0,x].data)\n",
    "                out.append(itos[x])\n",
    "                if x == 0:\n",
    "                    break\n",
    "                iix, ix = ix, x\n",
    "            names.append(''.join(out[:-1]))\n",
    "        return names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cycle #1, loss = 3.966810703277588\n"
     ]
    }
   ],
   "source": [
    "full_list_nn = Trigram_NN(names)\n",
    "full_list_nn.grad_descent(1, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cycle #1, loss = 2.809159517288208\n",
      "Cycle #2, loss = 2.808817148208618\n",
      "Cycle #3, loss = 2.8084754943847656\n",
      "Cycle #4, loss = 2.808134078979492\n",
      "Cycle #5, loss = 2.807793617248535\n",
      "Cycle #6, loss = 2.8074533939361572\n",
      "Cycle #7, loss = 2.8071134090423584\n",
      "Cycle #8, loss = 2.806774139404297\n",
      "Cycle #9, loss = 2.8064351081848145\n",
      "Cycle #10, loss = 2.8060972690582275\n",
      "Cycle #11, loss = 2.8057591915130615\n",
      "Cycle #12, loss = 2.805422306060791\n",
      "Cycle #13, loss = 2.8050854206085205\n",
      "Cycle #14, loss = 2.8047492504119873\n",
      "Cycle #15, loss = 2.804413318634033\n",
      "Cycle #16, loss = 2.8040783405303955\n",
      "Cycle #17, loss = 2.803743362426758\n",
      "Cycle #18, loss = 2.8034093379974365\n",
      "Cycle #19, loss = 2.8030753135681152\n",
      "Cycle #20, loss = 2.802741765975952\n",
      "Cycle #21, loss = 2.8024089336395264\n",
      "Cycle #22, loss = 2.802076578140259\n",
      "Cycle #23, loss = 2.8017444610595703\n",
      "Cycle #24, loss = 2.8014132976531982\n",
      "Cycle #25, loss = 2.8010823726654053\n",
      "Cycle #26, loss = 2.8007519245147705\n",
      "Cycle #27, loss = 2.800421953201294\n",
      "Cycle #28, loss = 2.8000924587249756\n",
      "Cycle #29, loss = 2.7997632026672363\n",
      "Cycle #30, loss = 2.7994351387023926\n",
      "Cycle #31, loss = 2.799107074737549\n",
      "Cycle #32, loss = 2.798779249191284\n",
      "Cycle #33, loss = 2.7984519004821777\n",
      "Cycle #34, loss = 2.7981252670288086\n",
      "Cycle #35, loss = 2.7977991104125977\n",
      "Cycle #36, loss = 2.797473430633545\n",
      "Cycle #37, loss = 2.7971479892730713\n",
      "Cycle #38, loss = 2.796823263168335\n",
      "Cycle #39, loss = 2.796499252319336\n",
      "Cycle #40, loss = 2.796175241470337\n",
      "Cycle #41, loss = 2.795851707458496\n",
      "Cycle #42, loss = 2.7955288887023926\n",
      "Cycle #43, loss = 2.7952065467834473\n",
      "Cycle #44, loss = 2.794884443283081\n",
      "Cycle #45, loss = 2.794562816619873\n",
      "Cycle #46, loss = 2.7942416667938232\n",
      "Cycle #47, loss = 2.7939212322235107\n",
      "Cycle #48, loss = 2.7936010360717773\n",
      "Cycle #49, loss = 2.793281316757202\n",
      "Cycle #50, loss = 2.792961835861206\n",
      "Cycle #51, loss = 2.792642831802368\n",
      "Cycle #52, loss = 2.7923245429992676\n",
      "Cycle #53, loss = 2.792006492614746\n",
      "Cycle #54, loss = 2.79168963432312\n",
      "Cycle #55, loss = 2.791372299194336\n",
      "Cycle #56, loss = 2.791055679321289\n",
      "Cycle #57, loss = 2.7907395362854004\n",
      "Cycle #58, loss = 2.790423631668091\n",
      "Cycle #59, loss = 2.7901086807250977\n",
      "Cycle #60, loss = 2.7897942066192627\n",
      "Cycle #61, loss = 2.7894797325134277\n",
      "Cycle #62, loss = 2.789165735244751\n",
      "Cycle #63, loss = 2.7888526916503906\n",
      "Cycle #64, loss = 2.788539171218872\n",
      "Cycle #65, loss = 2.788226842880249\n",
      "Cycle #66, loss = 2.787914514541626\n",
      "Cycle #67, loss = 2.7876031398773193\n",
      "Cycle #68, loss = 2.787292003631592\n",
      "Cycle #69, loss = 2.786980628967285\n",
      "Cycle #70, loss = 2.786670684814453\n",
      "Cycle #71, loss = 2.786360502243042\n",
      "Cycle #72, loss = 2.786051034927368\n",
      "Cycle #73, loss = 2.7857418060302734\n",
      "Cycle #74, loss = 2.785433530807495\n",
      "Cycle #75, loss = 2.785125255584717\n",
      "Cycle #76, loss = 2.784817695617676\n",
      "Cycle #77, loss = 2.7845098972320557\n",
      "Cycle #78, loss = 2.784203052520752\n",
      "Cycle #79, loss = 2.7838969230651855\n",
      "Cycle #80, loss = 2.78359055519104\n",
      "Cycle #81, loss = 2.783285140991211\n",
      "Cycle #82, loss = 2.782979965209961\n",
      "Cycle #83, loss = 2.78267502784729\n",
      "Cycle #84, loss = 2.7823710441589355\n",
      "Cycle #85, loss = 2.782066822052002\n",
      "Cycle #86, loss = 2.7817630767822266\n",
      "Cycle #87, loss = 2.7814602851867676\n",
      "Cycle #88, loss = 2.7811574935913086\n",
      "Cycle #89, loss = 2.780855178833008\n",
      "Cycle #90, loss = 2.7805535793304443\n",
      "Cycle #91, loss = 2.78025221824646\n",
      "Cycle #92, loss = 2.779951333999634\n",
      "Cycle #93, loss = 2.7796506881713867\n",
      "Cycle #94, loss = 2.779350519180298\n",
      "Cycle #95, loss = 2.779050588607788\n",
      "Cycle #96, loss = 2.7787511348724365\n",
      "Cycle #97, loss = 2.7784523963928223\n",
      "Cycle #98, loss = 2.778154134750366\n",
      "Cycle #99, loss = 2.7778561115264893\n",
      "Cycle #100, loss = 2.7775583267211914\n",
      "Cycle #101, loss = 2.7772610187530518\n",
      "Cycle #102, loss = 2.7769641876220703\n",
      "Cycle #103, loss = 2.776667594909668\n",
      "Cycle #104, loss = 2.776371717453003\n",
      "Cycle #105, loss = 2.776075839996338\n",
      "Cycle #106, loss = 2.7757809162139893\n",
      "Cycle #107, loss = 2.7754857540130615\n",
      "Cycle #108, loss = 2.77519154548645\n",
      "Cycle #109, loss = 2.774897813796997\n",
      "Cycle #110, loss = 2.774604082107544\n",
      "Cycle #111, loss = 2.774310350418091\n",
      "Cycle #112, loss = 2.774017810821533\n",
      "Cycle #113, loss = 2.773725748062134\n",
      "Cycle #114, loss = 2.7734336853027344\n",
      "Cycle #115, loss = 2.773141860961914\n",
      "Cycle #116, loss = 2.772850751876831\n",
      "Cycle #117, loss = 2.7725601196289062\n",
      "Cycle #118, loss = 2.7722694873809814\n",
      "Cycle #119, loss = 2.771979570388794\n",
      "Cycle #120, loss = 2.7716901302337646\n",
      "Cycle #121, loss = 2.7714006900787354\n",
      "Cycle #122, loss = 2.7711117267608643\n",
      "Cycle #123, loss = 2.7708234786987305\n",
      "Cycle #124, loss = 2.7705349922180176\n",
      "Cycle #125, loss = 2.7702476978302\n",
      "Cycle #126, loss = 2.769960403442383\n",
      "Cycle #127, loss = 2.7696735858917236\n",
      "Cycle #128, loss = 2.7693872451782227\n",
      "Cycle #129, loss = 2.769101142883301\n",
      "Cycle #130, loss = 2.768815040588379\n",
      "Cycle #131, loss = 2.7685298919677734\n",
      "Cycle #132, loss = 2.768244981765747\n",
      "Cycle #133, loss = 2.7679603099823\n",
      "Cycle #134, loss = 2.7676761150360107\n",
      "Cycle #135, loss = 2.76739239692688\n",
      "Cycle #136, loss = 2.767108917236328\n",
      "Cycle #137, loss = 2.7668259143829346\n",
      "Cycle #138, loss = 2.766543388366699\n",
      "Cycle #139, loss = 2.766261339187622\n",
      "Cycle #140, loss = 2.765979051589966\n",
      "Cycle #141, loss = 2.765697717666626\n",
      "Cycle #142, loss = 2.765416383743286\n",
      "Cycle #143, loss = 2.7651357650756836\n",
      "Cycle #144, loss = 2.764855146408081\n",
      "Cycle #145, loss = 2.7645750045776367\n",
      "Cycle #146, loss = 2.764296054840088\n",
      "Cycle #147, loss = 2.764016628265381\n",
      "Cycle #148, loss = 2.763737916946411\n",
      "Cycle #149, loss = 2.7634589672088623\n",
      "Cycle #150, loss = 2.76318097114563\n",
      "Cycle #151, loss = 2.7629032135009766\n",
      "Cycle #152, loss = 2.7626259326934814\n",
      "Cycle #153, loss = 2.7623488903045654\n",
      "Cycle #154, loss = 2.7620723247528076\n",
      "Cycle #155, loss = 2.761795997619629\n",
      "Cycle #156, loss = 2.7615203857421875\n",
      "Cycle #157, loss = 2.761244535446167\n",
      "Cycle #158, loss = 2.760969400405884\n",
      "Cycle #159, loss = 2.760694742202759\n",
      "Cycle #160, loss = 2.7604198455810547\n",
      "Cycle #161, loss = 2.760145902633667\n",
      "Cycle #162, loss = 2.7598719596862793\n",
      "Cycle #163, loss = 2.759599208831787\n",
      "Cycle #164, loss = 2.759326219558716\n",
      "Cycle #165, loss = 2.7590534687042236\n",
      "Cycle #166, loss = 2.7587807178497314\n",
      "Cycle #167, loss = 2.7585091590881348\n",
      "Cycle #168, loss = 2.758237600326538\n",
      "Cycle #169, loss = 2.7579662799835205\n",
      "Cycle #170, loss = 2.7576956748962402\n",
      "Cycle #171, loss = 2.75742506980896\n",
      "Cycle #172, loss = 2.757155179977417\n",
      "Cycle #173, loss = 2.756885290145874\n",
      "Cycle #174, loss = 2.7566161155700684\n",
      "Cycle #175, loss = 2.7563467025756836\n",
      "Cycle #176, loss = 2.7560782432556152\n",
      "Cycle #177, loss = 2.755810260772705\n",
      "Cycle #178, loss = 2.755542039871216\n",
      "Cycle #179, loss = 2.7552742958068848\n",
      "Cycle #180, loss = 2.755007266998291\n",
      "Cycle #181, loss = 2.754739761352539\n",
      "Cycle #182, loss = 2.7544734477996826\n",
      "Cycle #183, loss = 2.7542076110839844\n",
      "Cycle #184, loss = 2.753941535949707\n",
      "Cycle #185, loss = 2.753675937652588\n",
      "Cycle #186, loss = 2.753410577774048\n",
      "Cycle #187, loss = 2.753145694732666\n",
      "Cycle #188, loss = 2.7528812885284424\n",
      "Cycle #189, loss = 2.752617359161377\n",
      "Cycle #190, loss = 2.7523534297943115\n",
      "Cycle #191, loss = 2.752089738845825\n",
      "Cycle #192, loss = 2.7518270015716553\n",
      "Cycle #193, loss = 2.7515640258789062\n",
      "Cycle #194, loss = 2.7513017654418945\n",
      "Cycle #195, loss = 2.7510392665863037\n",
      "Cycle #196, loss = 2.7507777214050293\n",
      "Cycle #197, loss = 2.750516414642334\n",
      "Cycle #198, loss = 2.7502553462982178\n",
      "Cycle #199, loss = 2.7499947547912598\n",
      "Cycle #200, loss = 2.7497341632843018\n"
     ]
    }
   ],
   "source": [
    "full_list_nn.grad_descent(200, 0.25, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lari.',\n",
       " 'solinakimrarl.',\n",
       " 'ae.',\n",
       " 'mareeovzmnnamyra.',\n",
       " 'say.',\n",
       " 'jocfsuefon.',\n",
       " 'ka.',\n",
       " 'ztvy.',\n",
       " 'cozmcfzxvvie.',\n",
       " 'bjnkehah.',\n",
       " 'l.',\n",
       " 'gana.',\n",
       " 'can.',\n",
       " 'duso.',\n",
       " 'keriann.',\n",
       " 'liallgbqsgrotfibdkgrepnynnbywvk.',\n",
       " 'aaian.',\n",
       " 'dambhn.',\n",
       " 'lirlynn.',\n",
       " 'liy.']"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_list_nn.get_names(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E02: split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25626 3203 3204\n"
     ]
    }
   ],
   "source": [
    "# split dataset into 80:10:10 - train 2 and 3gram on training set - evaluate on dev and test\n",
    "import random\n",
    "random.shuffle(names)\n",
    "num_names = len(names)\n",
    "training_set = names[:int(0.8*num_names)]\n",
    "dev_set = names[int(0.8*num_names):int(0.9*num_names)]\n",
    "test_set = names[int(0.9*num_names):]\n",
    "print(len(training_set), len(dev_set), len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E03: use the dev set to tune the strength of smoothing (or regularization) for the trigram model - i.e. try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E04: we saw that our 1-hot vectors merely select a row of W, so producing these vectors explicitly feels wasteful. Can you delete our use of F.one_hot in favor of simply indexing into rows of W?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E05: look up and use F.cross_entropy instead. You should achieve the same result. Can you think of why we'd prefer to use F.cross_entropy instead?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E06: meta-exercise! Think of a fun/interesting exercise and complete it.\n",
    "\n",
    "\n",
    "Useful links for practice:\n",
    "- Python + Numpy tutorial from CS231n https://cs231n.github.io/python-numpy... . We use torch.tensor instead of numpy.array in this video. Their design (e.g. broadcasting, data types, etc.) is so similar that practicing one is basically practicing the other, just be careful with some of the APIs - how various functions are named, what arguments they take, etc. - these details can vary.\n",
    "- PyTorch tutorial on Tensor https://pytorch.org/tutorials/beginne...\n",
    "- Another PyTorch intro to Tensor https://pytorch.org/tutorials/beginne...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
